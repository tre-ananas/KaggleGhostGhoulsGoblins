levels = 5)
# Split data for cross-validation (CV)
rf_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
rf_cv_results <- rf_wf %>%
tune_grid(resamples = rf_folds,
grid = rf_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
rf_best_tune <- rf_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
rf_final_wf <- rf_wf %>%
finalize_workflow(rf_best_tune) %>%
fit(data = ggg_train)
# Predict class
rf_preds <- predict(rf_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Extract the final model
final_model <- rf_final_wf$fit$fit
final_model
# Best Tuning Parameters
rf_best_tune
# Ensure training data was balanced
table(ggg_train$type)
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
rf_cv_results
View(rf_cv_results)
View(rf_cv_results[[3]][[1]])
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=mtry, y=mean)) + geom_line()
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=min_n, y=mean)) + geom_line()
# Plot cross-validation
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=mtry, y=mean)) + geom_line()
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=min_n, y=mean)) + geom_line()
# Ensure training data was balanced
table(ggg_train$type)
# Predict class for training data
train_preds <- predict(rf_final_wf, new_data = ggg_train) %>%
bind_cols(ggg_train) %>%
rename(predicted_type = .pred_class)
# Identify misclassified samples
misclassified_samples <- train_preds %>%
filter(type != predicted_type) %>%
select(id, type, predicted_type)
# View misclassified samples with axes
print(misclassified_samples)
# View misclassified samples with axes
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Calculate the percentage of accurate predictions for each type
accurate_predictions <- table(rf_preds$type == ggg_test$type) / nrow(ggg_test) * 100
# Calculate the percentage of accurate predictions for each type
accurate_predictions <- table(rf_preds$type) / nrow(ggg_test) * 100
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct"),
col = c("red", "green"),
ylim = c(0, 100))
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct"),
col = c("red", "green"),
ylim = c(0, 100))
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct", "Incorrect2"),
col = c("red", "green"),
ylim = c(0, 100))
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Create a confusion matrix
confusion_matrix <- table(rf_preds$type, ggg_test$type)
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
library(caret)
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kernlab)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
ggg_train$color <- as.factor(ggg_train$color)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kernlab)
library(caret)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
ggg_train$color <- as.factor(ggg_train$color)
# Define the control parameters for the RFE
ctrl <- rfeControl(functions=rfFuncs, method="cv", number=10)
# Specify the features and the target variable
features <- c("bone_length", "rotting_flesh", "hair_length", "has_soul", "color")
target <- "type"  # Replace with your target variable name
# Create the RFE model
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=ctrl)
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
library(ggplot2)
# Create a data frame for plotting
feature_importance <- data.frame(
Feature = names(rfe_model$fit$bestSubset),
Importance = rfe_model$fit$coef
)
# Create a bar plot
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity") +
xlab("Feature") +
ylab("Importance") +
ggtitle("Feature Importance") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
feature_importance
# Create a data frame for plotting
feature_importance <- data.frame(
Feature = names(rfe_model$fit$bestSubset),
Importance = rfe_model$fit$coef
)
# Create a heatmap
ggplot(feature_importance, aes(x = Feature, y = 1, fill = Importance)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_void()
feature_importance
names(rfe_model$fit$bestSubset)
rfe_model$fit
rfe_model
View(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
install.packages('e1071')
library(e1071)
install.packages("e1071")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kernlab)
library(caret)
library(ggplot2)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
ggg_train$color <- as.factor(ggg_train$color)
# Define the control parameters for the RFE
ctrl <- rfeControl(functions=rfFuncs, method="cv", number=10)
# Specify the features and the target variable
features <- c("bone_length", "rotting_flesh", "hair_length", "has_soul", "color")
target <- "type"  # Replace with your target variable name
# Create the RFE model
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=ctrl)
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
install.packages("e1071")
install.packages("e1071")
library(e1071)
# Support Vector Machine - Linear
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10),
method="svmLinear")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
# Support Vector Machine - Linear
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10),
method="svmRadial")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
# Gradient Boosted Machines
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10),
method="gbm")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10),
method="lm")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
ggg_train$color <- as.factor(ggg_train$color)
# Define the control parameters for the RFE
ctrl <- rfeControl(functions=rfFuncs,
method="cv",
number=10,
selectionFunction = "best",
verbose = TRUE,
metric = "Accuracy")
# Define the control parameters for the RFE
ctrl <- rfeControl(functions=rfFuncs,
method="cv",
number=10,
verbose = TRUE)
# Specify the features and the target variable
features <- c("bone_length", "rotting_flesh", "hair_length", "has_soul", "color")
target <- "type"  # Replace with your target variable name
# Create the RFE model
# Random Forest
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=ctrl, metric = "Accuracy")
# Define the control parameters for the RFE
ctrl <- rfeControl(functions=rfFuncs,
method="cv",
number=10)
# Specify the features and the target variable
features <- c("bone_length", "rotting_flesh", "hair_length", "has_soul", "color")
target <- "type"  # Replace with your target variable name
# Create the RFE model
# Random Forest
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=ctrl, metric = "Accuracy")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
# Gradient Boosted Machines
rfe_model <- rfe(x = ggg_train[features], y = ggg_train[[target]], sizes=c(1:length(features)),
rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10),
method="gbm", metric = "Accuracy")
# Print the results
print(rfe_model)
# Get the selected features
selected_features <- rfe_model$optVariables
selected_features
# Recipe (leave out 'id')
svms_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_normalize(all_predictors())
svms_prep <- prep(svms_rec)
bake(svms_prep, ggg_train)
# Create linear SVMS model
svms_lin_mod <- svm_linear(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create radial SVMS model
svms_rad_mod <- svm_rbf(rbf_sigma = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create poly SVMS model
svms_poly_mod <- svm_poly(degree = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create linear SVMS workflow
svms_lin_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_lin_mod)
# Create radial SVMS workflow
svms_rad_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_rad_mod)
# Create poly SVMS workflow
svms_poly_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_poly_mod)
# Grid of values to tune over (linear)
svms_lin_tg <- grid_regular(cost(),
levels = 5)
# Grid of values to tune over (radial)
svms_rad_tg <- grid_regular(rbf_sigma(),
cost(),
levels = 5)
# Grid of values to tune over (poly)
svms_poly_tg <- grid_regular(degree(),
cost(),
levels = 5)
# Split data for cross-validation (CV) (linear)
svms_lin_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (radial)
svms_rad_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (poly)
svms_poly_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = svms_lin_folds,
grid = svms_lin_tg,
metrics = metric_set(accuracy))
# Run cross-validation (radial)
svms_rad_cv_results <- svms_rad_wf %>%
tune_grid(resamples = svms_rad_folds,
grid = svms_rad_tg,
metrics = metric_set(accuracy))
# Run cross-validation (poly)
svms_poly_cv_results <- svms_poly_wf %>%
tune_grid(resamples = svms_poly_folds,
grid = svms_poly_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters (linear)
svms_lin_best_tune <- svms_lin_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (radial)
svms_rad_best_tune <- svms_rad_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (poly)
svms_poly_best_tune <- svms_poly_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it (linear)
svms_lin_final_wf <- svms_lin_wf %>%
finalize_workflow(svms_lin_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (radial)
svms_rad_final_wf <- svms_rad_wf %>%
finalize_workflow(svms_rad_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (poly)
svms_poly_final_wf <- svms_poly_wf %>%
finalize_workflow(svms_poly_best_tune) %>%
fit(data = ggg_train)
# Predict class (linear)
svms_lin_preds <- predict(svms_lin_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (radial)
svms_rad_preds <- predict(svms_rad_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (poly)
svms_poly_preds <- predict(svms_poly_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (linear)
vroom_write(x=svms_lin_preds, file="svms_lin_preds_3.csv", delim = ",")
# Create a CSV with the predictions (radial)
vroom_write(x=svms_rad_preds, file="svms_rad_preds_4.csv", delim = ",")
# Create a CSV with the predictions (poly)
vroom_write(x=svms_poly_preds, file="svms_poly_preds_4.csv", delim = ",")
# Create linear SVMS model
svms_lin_mod <- svm_linear(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create radial SVMS model
svms_rad_mod <- svm_rbf(rbf_sigma = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create poly SVMS model
svms_poly_mod <- svm_poly(degree = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create linear SVMS workflow
svms_lin_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_lin_mod)
# Create radial SVMS workflow
svms_rad_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_rad_mod)
# Create poly SVMS workflow
svms_poly_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_poly_mod)
# Grid of values to tune over (linear)
svms_lin_tg <- grid_regular(cost(),
levels = 10)
# Grid of values to tune over (radial)
svms_rad_tg <- grid_regular(rbf_sigma(),
cost(),
levels = 10)
# Grid of values to tune over (poly)
svms_poly_tg <- grid_regular(degree(),
cost(),
levels = 10)
# Split data for cross-validation (CV) (linear)
svms_lin_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (radial)
svms_rad_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (poly)
svms_poly_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = svms_lin_folds,
grid = svms_lin_tg,
metrics = metric_set(accuracy))
# Run cross-validation (radial)
svms_rad_cv_results <- svms_rad_wf %>%
tune_grid(resamples = svms_rad_folds,
grid = svms_rad_tg,
metrics = metric_set(accuracy))
# Run cross-validation (poly)
svms_poly_cv_results <- svms_poly_wf %>%
tune_grid(resamples = svms_poly_folds,
grid = svms_poly_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters (linear)
svms_lin_best_tune <- svms_lin_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (radial)
svms_rad_best_tune <- svms_rad_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (poly)
svms_poly_best_tune <- svms_poly_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it (linear)
svms_lin_final_wf <- svms_lin_wf %>%
finalize_workflow(svms_lin_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (radial)
svms_rad_final_wf <- svms_rad_wf %>%
finalize_workflow(svms_rad_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (poly)
svms_poly_final_wf <- svms_poly_wf %>%
finalize_workflow(svms_poly_best_tune) %>%
fit(data = ggg_train)
# Predict class (linear)
svms_lin_preds <- predict(svms_lin_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (radial)
svms_rad_preds <- predict(svms_rad_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (poly)
svms_poly_preds <- predict(svms_poly_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (linear)
vroom_write(x=svms_lin_preds, file="svms_lin_preds_4.csv", delim = ",")
# Create a CSV with the predictions (radial)
vroom_write(x=svms_rad_preds, file="svms_rad_preds_5.csv", delim = ",")
# Create a CSV with the predictions (poly)
vroom_write(x=svms_poly_preds, file="svms_poly_preds_5.csv", delim = ",")
