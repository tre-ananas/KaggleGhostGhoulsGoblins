set_mode("classification")
# Create an XGBoost Workflow
xgb_wf <- workflow() %>%
add_recipe(xgb_rec) %>%
add_model(xgb_spec)
xgb_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(1, 10)),
min_n(range = c(1, 10)),
learn_rate(range = c(0.01, 0.1)),
levels = 10
)
# Split data for cross-validation (CV)
xgb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
xgb_cv_results <- xgb_wf %>%
tune_grid(resamples = xgb_folds,
grid = xgb_grid,
metrics = metric_set(accuracy))
# Find best tuning parameters
xgb_best_tune <- xgb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
xgb_final_wf <- xgb_wf %>%
finalize_workflow(xgb_best_tune) %>%
fit(data = ggg_train)
# Predict class
xgb_preds <- predict(xgb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=xgb_preds, file="xgb_preds_3.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(parsnip)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(terms = list(c("bone_length", "rotting_flesh"),
c("hair_length", "has_soul")))
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(terms = list(c("bone_length", "rotting_flesh"),
c("hair_length", "has_soul"))) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(term = list(c("bone_length", "rotting_flesh"),
c("hair_length", "has_soul"))) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(terms = formula(type ~
interaction(bone_length, rotting_flesh) +
interaction(hair_length, has_soul) +
interaction(hair_length, bone_length) +
interaction(rotting_flesh, has_soul))) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
step_interact(term = formula(type ~ interaction(bone_length, hair_length)) %>%
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(term = formula(type ~ interaction(bone_length, hair_length))) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_interact(terms = ~ hair_length:bone_length) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
bake(rf_prep, ggg_train)
x <p bake(rf_prep, ggg_train)
x <- bake(rf_prep, ggg_train)
x
View(x)
# Create Random Forest model specification
rf_spec <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
rf_wf <- workflow() %>%
add_recipe(rf_rec) %>%
add_model(rf_spec)
# Grid of values to tune over
rf_tg <- grid_regular(mtry(range = c(1, 5)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
rf_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
rf_cv_results <- rf_wf %>%
tune_grid(resamples = rf_folds,
grid = rf_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
rf_best_tune <- rf_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
rf_final_wf <- rf_wf %>%
finalize_workflow(rf_best_tune) %>%
fit(data = ggg_train)
# Predict class
rf_preds <- predict(rf_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=rf_preds, file="rf_preds_3.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(naivebayes)
library(discrim)
library(embed)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Create Recipe
nb_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCS Threshold = .8
step_pca(all_predictors(), threshold = .80)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCS Threshold = .8
step_pca(all_predictors(), threshold = .80)
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCS Threshold = .8
step_pca(all_predictors(), threshold = .80)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_mixed(color, outcome = vars(type)) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = .80)
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type)) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = .80)
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nbpcr_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = ggg_train)
# Predict class (Naive Bayes)
nb_preds <- predict(nb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds.csv", delim = ",")
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type)) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = .80)
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 10)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = ggg_train)
# Predict class (Naive Bayes)
nb_preds <- predict(nb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds_2.csv", delim = ",")
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type))
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 10)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = ggg_train)
# Predict class (Naive Bayes)
nb_preds <- predict(nb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds_3.csv", delim = ",")
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 10)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = ggg_train)
# Predict class (Naive Bayes)
nb_preds <- predict(nb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds_4.csv", delim = ",")
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
library(naivebayes)
library(discrim)
library(kernlab)
library(parsnip)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
View(ggg_train)
# RF Recipe
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# NB Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_normalize(all_numeric_predictors())
# SVMS Recipe
svms_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_dummy(color) %>%
step_normalize(all_predictors())
rf_prep <- prep(rf_rec)
# RF Recipe
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
bake(rf_prep, ggg_train)
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train)
# SVMS Recipe
svms_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_dummy(color) %>%
step_normalize(all_predictors())
svms_prep <- prep(svms_rec)
bake(svms_prep, ggg_train)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type)) %>%
step_normalize(all_numeric_predictors())
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(naivebayes)
library(discrim)
library(embed)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type)) %>%
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Recipe
nb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_lencode_glm(color, outcome = vars(type)) %>%
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, ggg_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = ggg_train)
# Predict class (Naive Bayes)
nb_preds <- predict(nb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds_5.csv", delim = ",")
