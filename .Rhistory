bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (linear)
vroom_write(x=svms_lin_preds, file="svms_lin_preds_3.csv", delim = ",")
# Create a CSV with the predictions (radial)
vroom_write(x=svms_rad_preds, file="svms_rad_preds_4.csv", delim = ",")
# Create a CSV with the predictions (poly)
vroom_write(x=svms_poly_preds, file="svms_poly_preds_4.csv", delim = ",")
# Create linear SVMS model
svms_lin_mod <- svm_linear(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create radial SVMS model
svms_rad_mod <- svm_rbf(rbf_sigma = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create poly SVMS model
svms_poly_mod <- svm_poly(degree = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create linear SVMS workflow
svms_lin_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_lin_mod)
# Create radial SVMS workflow
svms_rad_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_rad_mod)
# Create poly SVMS workflow
svms_poly_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_poly_mod)
# Grid of values to tune over (linear)
svms_lin_tg <- grid_regular(cost(),
levels = 10)
# Grid of values to tune over (radial)
svms_rad_tg <- grid_regular(rbf_sigma(),
cost(),
levels = 10)
# Grid of values to tune over (poly)
svms_poly_tg <- grid_regular(degree(),
cost(),
levels = 10)
# Split data for cross-validation (CV) (linear)
svms_lin_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (radial)
svms_rad_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Split data for cross-validation (CV) (poly)
svms_poly_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = svms_lin_folds,
grid = svms_lin_tg,
metrics = metric_set(accuracy))
# Run cross-validation (radial)
svms_rad_cv_results <- svms_rad_wf %>%
tune_grid(resamples = svms_rad_folds,
grid = svms_rad_tg,
metrics = metric_set(accuracy))
# Run cross-validation (poly)
svms_poly_cv_results <- svms_poly_wf %>%
tune_grid(resamples = svms_poly_folds,
grid = svms_poly_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters (linear)
svms_lin_best_tune <- svms_lin_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (radial)
svms_rad_best_tune <- svms_rad_cv_results %>%
select_best("accuracy")
# Find best tuning parameters (poly)
svms_poly_best_tune <- svms_poly_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it (linear)
svms_lin_final_wf <- svms_lin_wf %>%
finalize_workflow(svms_lin_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (radial)
svms_rad_final_wf <- svms_rad_wf %>%
finalize_workflow(svms_rad_best_tune) %>%
fit(data = ggg_train)
# Finalize workflow and fit it (poly)
svms_poly_final_wf <- svms_poly_wf %>%
finalize_workflow(svms_poly_best_tune) %>%
fit(data = ggg_train)
# Predict class (linear)
svms_lin_preds <- predict(svms_lin_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (radial)
svms_rad_preds <- predict(svms_rad_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Predict class (poly)
svms_poly_preds <- predict(svms_poly_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (linear)
vroom_write(x=svms_lin_preds, file="svms_lin_preds_4.csv", delim = ",")
# Create a CSV with the predictions (radial)
vroom_write(x=svms_rad_preds, file="svms_rad_preds_5.csv", delim = ",")
# Create a CSV with the predictions (poly)
vroom_write(x=svms_poly_preds, file="svms_poly_preds_5.csv", delim = ",")
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kknn)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
knn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_normalize(all_numeric_predictors())
knn_prep <- prep(knn_rec)
bake(knn_prep, ggg_train)
# Create KNN model specification
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_mode("classification") %>%
set_engine("kknn")
# Create KNN workflow
knn_wf <- workflow() %>%
add_recipe(knn_rec) %>%
add_model(knn_mod)
# Grid of values to tune over
knn_tg <- grid_regular(neighbors(),
levels = 5)
# Split data for cross-validation (CV)
knn_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Split data for cross-validation (CV)
knn_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
knn_cv_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
knn_best_tune <- knn_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = ggg_train)
# Predict class
knn_preds <- predict(knn_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=knn_preds, file="knn_preds.csv", delim = ",")
# Grid of values to tune over
knn_tg <- grid_regular(neighbors(),
levels = 10)
# Split data for cross-validation (CV)
knn_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
knn_cv_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
knn_best_tune <- knn_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = ggg_train)
# Predict class
knn_preds <- predict(knn_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=knn_preds, file="knn_preds_1.csv", delim = ",")
# Load Data
ggg_train <- vroom("train.csv")
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
svms_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_dummy(color) %>%
step_normalize(all_predictors())
svms_prep <- prep(svms_rec)
bake(svms_prep, ggg_train)
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(employee_train,
v = 10,
repeats = 1) # Split data for CV
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(ggg_train,
v = 10,
repeats = 1) # Split data for CV
stack_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(ggg_train,
v = 10,
repeats = 1) # Split data for CV
stack_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" and "color" into factors
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
svms_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_mutate_at(color, fn = factor) %>%
step_dummy(color) %>%
step_normalize(all_predictors())
svms_prep <- prep(svms_rec)
bake(svms_prep, ggg_train)
# Set up cross validation
stack_folds <- vfold_cv(ggg_train,
v = 10,
repeats = 1) # Split data for CV
stack_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# Create linear SVMS model
svms_lin_mod <- svm_linear(cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create radial SVMS model
svms_rad_mod <- svm_rbf(rbf_sigma = tune(),
cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create linear SVMS workflow
svms_lin_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_lin_mod)
# Create radial SVMS workflow
svms_rad_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_rad_mod)
# Grid of values to tune over (linear)
svms_lin_tg <- grid_regular(cost(),
levels = 5)
# Grid of values to tune over (radial)
svms_rad_tg <- grid_regular(rbf_sigma(),
cost(),
levels = 10)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = svms_lin_folds,
grid = svms_lin_tg,
metrics = metric_set(accuracy),
control = stack_untuned_model)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = stack_folds,
grid = svms_lin_tg,
metrics = metric_set(accuracy),
control = stack_untuned_model)
# Run cross-validation (radial)
svms_rad_cv_results <- svms_rad_wf %>%
tune_grid(resamples = stack_folds,
grid = svms_rad_tg,
metrics = metric_set(accuracy),
control = stack_untuned_model)
# Specify models to include in stacked model
stack_stack <- stacks() %>%
add_candidates(svms_lin_cv_results) %>%
add_candidates(svms_rad_cv_results)
# Fit model w/ LASSO penalized regression meta-learner
stacked_model <- stack_stack %>%
blend_predictions() %>%
fit_members()
# Specify models to include in stacked model
stack_stack <- stacks() %>%
add_candidates(svms_lin_cv_results) %>%
add_candidates(svms_rad_cv_results)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = stack_folds,
grid = svms_lin_tg,
metrics = metric_set(rpc_auc),
control = stack_untuned_model)
# Run cross-validation (linear)
svms_lin_cv_results <- svms_lin_wf %>%
tune_grid(resamples = stack_folds,
grid = svms_lin_tg,
metrics = metric_set(roc_auc),
control = stack_untuned_model)
# Run cross-validation (radial)
svms_rad_cv_results <- svms_rad_wf %>%
tune_grid(resamples = stack_folds,
grid = svms_rad_tg,
metrics = metric_set(roc_auc),
control = stack_untuned_model)
# Specify models to include in stacked model
stack_stack <- stacks() %>%
add_candidates(svms_lin_cv_results) %>%
add_candidates(svms_rad_cv_results)
# Specify models to include in stacked model
stack_stack <- stacks() %>%
add_candidates(svms_lin_cv_results) %>%
add_candidates(svms_rad_cv_results)
# Fit model w/ LASSO penalized regression meta-learner
stacked_model <- stack_stack %>%
blend_predictions() %>%
fit_members()
# Predict class (stacked)
stacked_preds <- predict(stacked_model,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (linear)
vroom_write(x=stacked_preds, file="stacked_preds.csv", delim = ",")
install.packages("bonsai")
install.packages("lightgbm")
library(vroom)
library(tidyverse)
library(tidymodels)
library(bonsai)
library(lightgbm)
library(embed)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
bst_brt_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_lencode_glm(color, outcome = vars(type)) %>% # glm allows you to target encode a factor on a factor
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
bst_brt_rec <- prep(bst_brt_rec)
bake(bst_brt_rec, ggg_train)
# Create a Boost model specification
bst_spec <- boost_tree(trees = tune(), tree_depth = tune(), learn_rate = tune()) %>%
set_engine("lightgbm") %>%
set_mode("classification")
# Create a Boost Workflow
bst_wf <- workflow() %>%
add_recipe(bst_brt_rec) %>%
add_model(bst_spec)
bst_grid <- grid_regular(
trees(range = c(500, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.1)),
levels = 5
)
# Split data for cross-validation (CV)
bst_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
bst_cv_results <- bst_wf %>%
tune_grid(resamples = bst_folds,
grid = bst_grid,
metrics = metric_set(accuracy))
# Find best tuning parameters
bst_best_tune <- bst_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
bst_final_wf <- bst_wf %>%
finalize_workflow(bst_best_tune) %>%
fit(data = ggg_train)
# Predict class
bst_preds <- predict(bst_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=bst_preds, file="bst_preds.csv", delim = ",")
# Create a BART model specification
bart_spec <- bart(trees = tune()) %>%
set_engine("dbarts") %>%
set_mode("classification")
# BART workflow
bart_wf <- workflow() %>%
add_recipe(bst_brt_rec) %>%
add_model(bart_spec)
# Set up BART tuning grid
bart_grid <- grid_regular(
trees(range = c(500, 1000),
levels = 5)
)
# Set up BART tuning grid
bart_grid <- grid_regular(
trees(range = c(500, 1000)),
levels = 5)
# Set up BART tuning grid
bart_grid <- grid_regular(
trees(range = c(500, 1000)),
levels = 5)
# Split data for cross-validation (CV)
bart_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Find best tuning parameters
bst_best_tune <- bst_cv_results %>%
select_best("accuracy")
# Run cross-validation for bart
bart_cv_results <- bart_wf %>%
tune_grid(resamples = bart_folds,
grid = bart_grid,
metrics = metric_set(accuracy))
# Find best tuning parameters (bart)
bart_best_tune <- bart_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it (bart)
bart_final_wf <- bart_wf %>%
finalize_workflow(bart_best_tune) %>%
fit(data = ggg_train)
# Predict class (Bart)
bart_preds <- predict(bart_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions (bart)
vroom_write(x=bart_preds, file="bart_preds.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(xgboost)
library(embed)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
xgb_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_lencode_glm(color, outcome = vars(type))# glm allows you to target encode a factor on a factor
xgb_prep <- prep(xgb_rec)
bake(xgb_prep, ggg_train)
# Create an XGBopst model specification
xgb_spec <- boost_tree(trees = tune(), tree_depth = tune(), min_n = tune(), learn_rate = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Create an XGBoost Workflow
xgb_wf <- workflow() %>%
add_recipe(xgb_rec) %>%
add_model(xgb_spec)
xgb_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(1, 10)),
min_n(range = c(1, 10)),
learn_rate(range = c(0.01, 0.1)),
levels = 10
)
# Split data for cross-validation (CV)
xgb_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
xgb_cv_results <- xgb_wf %>%
tune_grid(resamples = xgb_folds,
grid = xgb_grid,
metrics = metric_set(accuracy))
# Find best tuning parameters
xgb_best_tune <- xgb_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
xgb_final_wf <- xgb_wf %>%
finalize_workflow(xgb_best_tune) %>%
fit(data = ggg_train)
# Predict class
xgb_preds <- predict(xgb_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Create a CSV with the predictions
vroom_write(x=xgb_preds, file="xgb_preds_3.csv", delim = ",")
