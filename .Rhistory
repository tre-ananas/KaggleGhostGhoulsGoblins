step_impute_knn(bone_length,
impute_with = all_predictors(),
neighbors = 5)
# Prep, Bake, and View Recipe
imp_prep <- prep(imp_rec)
data.frame(na_count)
# Impute Missing Values
# Create Recipe
imp_rec <- recipe(ACTION ~ ., data = ggg_train_na) %>%
step_impute_knn(var = hair_length,
impute_with = c('has_soul',
'color'),
neighbors = 5) %>%
step_impute_knn(var = rotting_flesh,
impute_with = c('has_soul',
'color',
'hair_length'),
neighbors = 5) %>%
step_impute_knn(var = bone_length,
impute_with = c('has_soul',
'color',
'rotting_flesh',
'hair_length'),
neighbors = 5)
# Impute Missing Values
# Create Recipe
imp_rec <- recipe(type ~ ., data = ggg_train_na) %>%
step_impute_knn(var = hair_length,
impute_with = c('has_soul',
'color'),
neighbors = 5) %>%
step_impute_knn(var = rotting_flesh,
impute_with = c('has_soul',
'color',
'hair_length'),
neighbors = 5) %>%
step_impute_knn(var = bone_length,
impute_with = c('has_soul',
'color',
'rotting_flesh',
'hair_length'),
neighbors = 5)
# Prep, Bake, and View Recipe
imp_prep <- prep(imp_rec)
# Impute Missing Values
# Create Recipe
imp_rec <- recipe(type ~ ., data = ggg_train_na) %>%
step_impute_knn(var = hair_length,
impute_with = c('has_soul',
'color'),
neighbors = 5) %>%
step_impute_knn(var = rotting_flesh,
impute_with = c('has_soul',
'color'),
neighbors = 5) %>%
step_impute_knn(var = bone_length,
impute_with = c('has_soul',
'color'),
neighbors = 5)
# Prep, Bake, and View Recipe
imp_prep <- prep(imp_rec)
rlang::last_trace()
data.frame(na_count)
hist(ggg_train_na$hair_length)
hist(ggg_train_na$rotting_flesh)
hist(ggg_train_na$bone_length)
# Impute Missing Values
# Create Recipe
imp_rec <- recipe(type ~ ., data = ggg_train_na) %>%
step_impute_mean(hair_length) %>%
step_impute_mean(rotting_flesh) %>%
step_impute_mean(bone_length)
# Prep, Bake, and View Recipe
imp_prep <- prep(imp_rec)
bake(imp_prep, ggg_train_na) %>%
slice(1:10)
baked_train_na <- bake(imp_prep, ggg_train_na)
baked_train_na
# Caluclate RMSE
rmse_vec(baked_train_na[is.na(ggg_train_na)], imputedSet[is.na(ggg_train_na)])
imp_train <- bake(imp_prep, ggg_train_na)
# Caluclate RMSE
rmse_vec(ggg_train[is.na(ggg_train_na)], imp_train[is.na(ggg_train_na)])
table(ggg_train$type)
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(parsnip)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
bake(rf_prep, ggg_train)
# Create Random Forest model specification
rf_spec <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
rf_wf <- workflow() %>%
add_recipe(rf_rec) %>%
add_model(rf_spec)
# Grid of values to tune over
rf_tg <- grid_regular(mtry(range = c(1, 5)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
rf_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
rf_cv_results <- rf_wf %>%
tune_grid(resamples = rf_folds,
grid = rf_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
rf_best_tune <- rf_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
rf_final_wf <- rf_wf %>%
finalize_workflow(rf_best_tune) %>%
fit(data = ggg_train)
# Predict class
rf_preds <- predict(rf_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Extract the final model
final_model <- rf_final_wf$fit$fit
final_model
table(ggg_train$type)
# Predict class for training data
train_preds <- predict(rf_final_wf, new_data = ggg_train) %>%
bind_cols(ggg_train) %>%
rename(predicted_type = .pred_class)
# Identify misclassified samples
misclassified_samples <- train_preds %>%
filter(type != predicted_type)
# View misclassified samples
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Examine Results for Potential Areas to Improve
# Extract the final model
final_model <- rf_final_wf$fit$fit
final_model
# View Bes
rf_best_tune
# Ensure training data was balanced
table(ggg_train$type)
# View misclassified samples
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Identify misclassified samples
misclassified_samples <- train_preds %>%
filter(type != predicted_type)
# View misclassified samples
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Predict class for training data
train_preds <- predict(rf_final_wf, new_data = ggg_train) %>%
bind_cols(ggg_train) %>%
rename(predicted_type = .pred_class)
# Identify misclassified samples
misclassified_samples <- train_preds %>%
filter(type != predicted_type) %>%
select(id, type, predicted_type)
# View misclassified samples with axes
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# View misclassified samples with axes
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
misclassified_samples
print(misclassified_samples, n = 32)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
sum(misclassified_samples$type == "Goblin")
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
# Load Libraries
library(vroom)
library(tidyverse)
(vroom)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
nn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_num2factor(color) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_range(all_numeric_predictors(), min = 0, max = 1) # Scale Xs to [0, 1]
str(ggg_train)
unique(ggg_train$color)
# Recipe (leave out 'id')
nn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_num2factor(color, levels = c('clear',
'green',
'black',
'white',
'blue',
'blood')) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_range(all_numeric_predictors(), min = 0, max = 1) # Scale Xs to [0, 1]
nn_prep <- prep(nn_rec)
# Recipe (leave out 'id')
nn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_num2factor(color, levels = c('clear',
'green',
'black',
'white',
'blue',
'blood')) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_range(all_numeric_predictors(), min = 0, max = 1) # Scale Xs to [0, 1]
nn_prep <- prep(nn_rec)
# Recipe (leave out 'id')
nn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_num2factor(color, levels = c('clear',
'green',
'black',
'white',
'blue',
'blood')) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_range(all_numeric_predictors(), min = 0, max = 1) # Scale Xs to [0, 1]
nn_prep <- prep(nn_rec)
# Recipe (leave out 'id')
nn_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color)
nn_prep <- prep(nn_rec)
bake(nn_prep, ggg_train)
unique(ggg_train$color)
# Create Neural Network model specification
nn_spec <- mlp(hidden_units = tune(),
epochs = 100,
activation = "relu") %>%
set_engine("keras", verbose = 0) %>% # verbose = 0 prints off less output
set_mode('classification')
# Create classification NN workflow
nn_wf <- workflow() %>%
add_recipe(nn_rec) %>%
add_model(nn_spec)
# Grid of values to tune over
nn_tg <- grid_regular(hidden_units(range = c(1, 5)),
levels = 5)
# Grid of values to tune over
nn_tg <- grid_regular(hidden_units(range = c(1, 5)),
levels = 5)
# Split data for cross-validation (CV)
nn_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nn_cv_results <- nn_wf %>%
tune_grid(resamples = nn_folds,
grid = nn_tg,
metrics = metric_set(accuracy))
# Create Neural Network model specification
nn_spec <- mlp(hidden_units = tune(),
epochs = 100) %>%
set_engine("nnet") %>%
set_mode('classification')
# Create classification NN workflow
nn_wf <- workflow() %>%
add_recipe(nn_rec) %>%
add_model(nn_spec)
# Grid of values to tune over
nn_tg <- grid_regular(hidden_units(range = c(1, 5)),
levels = 5)
# Split data for cross-validation (CV)
nn_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nn_cv_results <- nn_wf %>%
tune_grid(resamples = nn_folds,
grid = nn_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
nn_best_tune <- nn_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
nn_final_wf <- nn_wf %>%
finalize_workflow(nn_best_tune) %>%
fit(data = ggg_train)
View(nn_cv_results)
# Create Neural Network model specification
nn_spec <- mlp(hidden_units = tune(),
epochs = 100) %>%
set_engine("nnet") %>%
set_mode('classification')
# Create classification NN workflow
nn_wf <- workflow() %>%
add_recipe(nn_rec) %>%
add_model(nn_spec)
# Grid of values to tune over
nn_tg <- grid_regular(hidden_units(range = c(1, 5)),
levels = 5)
# Split data for cross-validation (CV)
nn_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
nn_cv_results <- nn_wf %>%
tune_grid(resamples = nn_folds,
grid = nn_tg,
metrics = metric_set(accuracy))
nn_cv_results
View(nn_cv_results)
# Predict class
nn_preds <- predict(nn_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
nn_preds
plot(hidden_units, mean(accuracy))
nn_final_wf
View(nn_final_wf)
View(nn_cv_results)
View(nn_cv_results)
View(nn_cv_results[[3]][[4]])
View(nn_cv_results[[3]][[1]])
View(nn_cv_results[[3]][[2]])
View(nn_cv_results[[3]][[3]])
View(nn_cv_results[[3]][[4]])
View(nn_cv_results[[3]][[5]])
View(nn_best_tune)
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$splits
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$metrics
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1, 1]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1][1]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1, 3]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results$.metrics[1]
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
nn_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
# Create a CSV with the predictions
vroom_write(x=nn_preds, file="nn_preds.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidyverse)
library(tidymodels)
library(parsnip)
# Load Data
ggg_train <- vroom("train.csv")
ggg_test <- vroom("test.csv")
# Turn "type" into factor
ggg_train$type <- as.factor(ggg_train$type)
# Recipe (leave out 'id')
rf_rec <- recipe(type ~ bone_length + rotting_flesh + hair_length + has_soul + color, data = ggg_train) %>%
step_dummy(color) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rf_prep <- prep(rf_rec)
bake(rf_prep, ggg_train)
# Create Random Forest model specification
rf_spec <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
rf_wf <- workflow() %>%
add_recipe(rf_rec) %>%
add_model(rf_spec)
# Grid of values to tune over
rf_tg <- grid_regular(mtry(range = c(1, 5)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
rf_folds <- vfold_cv(ggg_train, v = 5, repeats = 1)
# Run cross-validation
rf_cv_results <- rf_wf %>%
tune_grid(resamples = rf_folds,
grid = rf_tg,
metrics = metric_set(accuracy))
# Find best tuning parameters
rf_best_tune <- rf_cv_results %>%
select_best("accuracy")
# Finalize workflow and fit it
rf_final_wf <- rf_wf %>%
finalize_workflow(rf_best_tune) %>%
fit(data = ggg_train)
# Predict class
rf_preds <- predict(rf_final_wf,
new_data = ggg_test,
type = "class") %>%
bind_cols(ggg_test$id, .) %>%
rename(type = .pred_class) %>%
rename(id = ...1) %>%
select(id, type)
# Extract the final model
final_model <- rf_final_wf$fit$fit
final_model
# Best Tuning Parameters
rf_best_tune
# Ensure training data was balanced
table(ggg_train$type)
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
rf_cv_results
View(rf_cv_results)
View(rf_cv_results[[3]][[1]])
# Plot cross-validation with hidden_units on the x-axis and mean(accuracy) on the y-axis for class
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=mtry, y=mean)) + geom_line()
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=min_n, y=mean)) + geom_line()
# Plot cross-validation
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=mtry, y=mean)) + geom_line()
rf_cv_results %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=min_n, y=mean)) + geom_line()
# Ensure training data was balanced
table(ggg_train$type)
# Predict class for training data
train_preds <- predict(rf_final_wf, new_data = ggg_train) %>%
bind_cols(ggg_train) %>%
rename(predicted_type = .pred_class)
# Identify misclassified samples
misclassified_samples <- train_preds %>%
filter(type != predicted_type) %>%
select(id, type, predicted_type)
# View misclassified samples with axes
print(misclassified_samples)
# View misclassified samples with axes
print(misclassified_samples)
# Count misclassified samples for each type
table(misclassified_samples$type, misclassified_samples$predicted_type)
# Calculate the percentage of accurate predictions for each type
accurate_predictions <- table(rf_preds$type == ggg_test$type) / nrow(ggg_test) * 100
# Calculate the percentage of accurate predictions for each type
accurate_predictions <- table(rf_preds$type) / nrow(ggg_test) * 100
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct"),
col = c("red", "green"),
ylim = c(0, 100))
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct"),
col = c("red", "green"),
ylim = c(0, 100))
# Bar plot for accurate predictions
barplot(accurate_predictions,
main = "Accuracy by Type",
xlab = "Correctly Predicted",
ylab = "Percentage",
names.arg = c("Incorrect", "Correct", "Incorrect2"),
col = c("red", "green"),
ylim = c(0, 100))
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
# Create a confusion matrix
confusion_matrix <- table(rf_preds$type, ggg_test$type)
# Calculate misclassification percentages
misclassification_percentages <- prop.table(confusion_matrix, margin = 1) * 100
